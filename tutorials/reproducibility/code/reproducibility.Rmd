---
title: "Reproducibility of data analysis"
author: "Stephanie Palmer & Stefano Allesina & Graham Smith"
date: "September 12-14, 2017"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
urlcolor: blue
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  eval      = FALSE,
  comment   = "#",
  results   = "hold",
  # collapse  = TRUE,
  fig.align = "center")
```



## Installation notes
For this tutorial, relatively up-to-date versions of `R` and `RStudio` are needed. To install `R`, follow instructions at [`cran.rstudio.com`](https://cran.rstudio.com/). Then install `Rstudio` following the instructions at [`goo.gl/a42jYE`](https://www.rstudio.com/products/rstudio/download2/). Download the `ggplot2`, `cowplot`, `stats`, and `RMKdiscrete` packages. You should also install the UNIX utility `git` for [OSX](https://git-scm.com/download/mac) or [Windows](https://git-for-windows.github.io/).

# Goals
This tutorial will cover methods for making your code reproducible, both by you and by others.
Producing results that are reproducible by others is the very essence of science, and writing code that is reproducible by **you** is just the first step.
In particular, we will discuss methods to ensure that you can:

* track changes to your code,
* make your code freely accessible to the wider world via [github](http://github.com/),
* make your code readable,
* reproduce your calculations precisely,

Along the way, you will get an introduction to stochastic processes and how they are used to model biological variability.
By the end of this tutorial, you should know why it is important to save your seeds and merge your branches.
You should also know why reproducible coding practices can help you *now*, even if (indeed, particularly if) you are just learning to code.

# What is reproducibility?

Here we'll consider three levels of reproducibility:

0. Accessibility
1. Replicability
2. Reusability
3. Extensibility

These are hierarchically ordered in necessity, difficulty, and goodness.

Reproducible code must be accessible to other researchers. However, accessible code is not necessarily reproducible in any sense. If the code is incomprehensible OR doesn't run, then you may as well not have published anything at all.

The first real level of reproducibility is replicability: Does your code run and produce the result you said it would produce? If yes, then at least your result is replicable. If no, then you haven't done science, you've done magic. In an ideal world, your reviewers would catch this.

Most scientists would probably aspire to reusability, the second level of reproducibility: can another scientist take your code and apply it to their own data? In order to apply your code correctly to their own data, another scientist will need to understand your code quite well. Therefore reusability requires more understandable code.

Finally, extensibility requires that your code be reconfigurable to address either different questions or different data. This is fundamentally different from the preceding, and is often beyond the scope of scientific inquiry. If you've written extensible code, you've made a tool, so this is more akin to methods development.

# Accessibility

## Why use Version Control?

- If you ever collaborate on writing code, then version control is for you.
- If you ever horribly break your project and need to move quickly back in time two days (or two weeks), then version control is for you.
- If you ever need to replicate a figure you made three months ago, version control is for you (I *guarantee* you will need to do this at some point).
- If you ever need to share your code publicly, then version control is for you.
- Most likely, version control is for you.

Version control is useful for small projects, and is essential for large collaborative projects. It vastly improves the workflow, efficiency and reproducibility. Without it, it is quite easy to lose track of the status of a manuscript (who has the most recent version?), or lose time (I cannot proceed with my changes, because I have to wait for the edits of my collaborators).

Many scientists keep backup versions of the same project over time or append a date/initials to different versions of the same file (e.g. various drafts of the same manuscript). This manual approach quickly becomes unmanageable, with simply too many files and versions to keep track of in a timely and organized manner. Version control allows you to access all previously committed versions of the files and directories of your project. This means that it is quite easy to undo short-term changes: Bad day? Just go back to yesterday's version! You can also access previous stages of the project: "I need to access the manuscript's version and all the analysis files in exactly the state that they where in when I sent the draft for review three months ago." Checking out an entire project at a certain point in time is easy with a version control system but much more difficult with Dropbox or Google Drive.

Version control makes it trivial to host your code publicly (e.g. on Github or Bitbucket) and to share a robust link to your code in any publication.

> Stefano's testimonial: "Our laboratory adopted version control for all our projects in 2010, and sometimes we wonder how we managed without it."


## Introduction to `git`
`git` is a version control system. Version control is a way to record and organize changes to a project that consists of files and directories. Over time, a version control system like `git` builds a repository of the whole history of your project.

When you start working on a new project, you tell `git` what directory will contain the project. Then `git` takes a snapshot of that directory to create the beginning of your repository. If the project is brand new, this may be a snapshot of an empty directory.  After you've done some work, you can tell `git` to take a new snapshot, called a **commit**. All snapshots remain available---you can always recover previously committed versions of files.

`git` is especially important for collaborative projects: everybody can simultaneously work on the project, even on the same file. Conflicting changes are reported, and can be managed using side-by-side comparisons. The possibility of **branching** allows you to experiment with changes (e.g. shall we rewrite the introduction of this paper?), and then decide whether to **merge** them into the project.







<!-- ## Git
For this introduction to version control, we use `git`, which is one of the most popular version control systems. `git` is also free software, and is available for all computer architectures. Many good tutorials are available online, and many websites will host your repositories for free.

Other options that exist are `Mercurial` (very similar to `git`) and Subversion Version Control `svn`, which is an older system, but still widespread.

There are two main paradigms for VCSs allowing multiple users to collaborate: in a *centralized* VCS (e.g., `svn`), the whole history of a project is stored exclusively on a server, and users download the most current snapshot of the project, modify it, and send the changes to the server; in a *distributed* VCS (e.g., `git`), the complete history of the repository is stored on each user's computer.

`git` was initially developed by Linus Torvalds (the "Linu" in Linux), exactly for the development of the Linux kernel. It was first released in 2005 and has since become the most widely adopted version control system.

At first, we work with *local* repositories, meaning that all the files are stored exclusively on your computer. `git` by default works with local repositories. Later, we introduce *remote* repositories, which are also hosted on a web server, making it easy for you to share your projects with others (or work on the same project from different computers). These remote repositories can be hosted on such well known sites as Github and Bitbucket, and `git` will play nicely. -->

<!-- ### Configuring git
First, open a terminal window.

The first time you use `git`, (or whenever you install `git` on a new computer), you need to set up the environment. To store your preferred user name and email, type:

```{r, engine = 'bash', eval = FALSE}
$ git config --global user.name "Charles Darwin"
$ git config --global user.email crdarwin@royalsociety.org
```

You can set up your preferred text editor, which will be used to write the messages associated with your commits. If you are familiar with text editors and already have a favorite (e.g. `emacs` or `vim`), feel free to use it. Otherwise, Window users please use `notepad` and Mac users please use `nano`.

```{r, engine = 'bash', eval = FALSE}
$ git config --global core.editor nano
```
Again, Windows users please replace `nano` with `notepad` in the above.

To check all of your settings and see all available options, type:
```{r, engine = 'bash', eval = FALSE}
$ git config --list
``` -->

## Help using GitKraken and `git`

GitKraken is a graphical user interface (GUI) on top of the older command-line interface (CLI) `git`. If you've never used `git` before, GitKraken has a good tutorial: [https://support.gitkraken.com/start-here/guide](https://support.gitkraken.com/start-here/guide)

## Glossary

  - `commit`: *v.* Take a snapshot of the current progress. *n.* A snapshot of the project at a particular point in time
  - `stage`: *v.* Add changes to the set that will be committed. (does not commit!)
  - `repository` (`repo`): *n.* Entire history of project. Set of all commits on all branches.
  - `branch`: *n.* A lineage of commits. A repository can have multiple branches, and committing changes to one will not affect the others. You can switch between branches (see `checkout`). When you switch branches, all your project's files change to the state they were in at the last commit on that branch. *v.* To create a new branch.
  - `checkout`: *v.* switch branches.
  - `pull`: *v.* download the latest version of the project from the remote repository.
  - `push`: *v.* upload your latest local commit to the remote repository.
  - `init`: *v.* Create an empty repository.
  - `clone`: *v.* Create a new local copy of a remote repository.


## Exercise 1: Your First Repo

1) Let's `init` your first repo from scratch! To do so, in GitKraken select `File > Init Repo`. We'll be creating a "Local Only" repository for this simple example. That just means we'll not be putting any files on any website (such as GitHub). The files are only stored on your computer. Browse for `Documents` or your equivalent. Note that you cannot put a `git` repository inside another `git` repository.

2) Now we have an empty repository. Let's put something in it. Using a text editor of your choice (Notepad, TextEdit, Sublime, even RStudio), create a file called `origin.txt` with the text "An abstract of an Essay on the Origin of Species..." and save it.

3) Now go back to GitKraken. At the top of the center panel, an entry has been added, `// WIP`. This stands for "Work In Progress" and it indicates you've made changes to your project that have not been committed to the repository. We have two steps to record the changes to the repository: First, we stage the changes. Second, we commit the staged changes.

4) Stage the change.

5) Type a commit message (e.g. "Began Essay on the Origin of Species") and commit the staged change.

If you use `git` in the wild, you'll find that staging and committing is 90% of what you do.

## Remote repositories

For the remainder of the tutorial, we'll be using a more complicated example based in code hosted on [GitHub](http://github.com). So far, we have been working with a local repository, meaning the repository is hosted only on your computer. Usually, you will also want to keep a copy of the repository online, called a ``remote repository.'' With a remote repository, you can collaborate with others, sync your code across multiple machines, and back up your code.

The most popular option for hosting remote repositories is [GitHub](http://github.com). As a student you can get private repositories for free! The [Student Developer Pack](https://education.github.com/pack) comes with access to lots of other goodies, but private repositories make it a necessity. They're useful in the early stages of your project when you're paranoid about anyone, let alone the entire world, seeing your hasty hacks. But remember to publicize your repository when you publish! (Note: probably still not HIPAA compliant)

After the initial setup, you only need to add two new commands to your `git` workflow: `pull` and `push`. When you want to work on a project that is tracking a remote repository, you `pull` the most recent version from the server to sync your local copy of the project to the most recent version. When you are done working, you `push` your commits to the server so that other users can see them.

## Daily Workflow

 1) Pull any new changes from your collaborators
 2) Work on your project
 3) When you've done something meaningful, stage your changes
 4) Commit your changes, writing a meaningful commit message
 5) Repeat steps 2-4
 6) When you're done for the day, or when you've finished code that you want your collaborators to be able to access, push your changes to the remote repository (e.g. on GitHub).

## Clone a Repo

[TODO: common use case: use someone else's code]

[TODO: clone LotsOBugs repo]


# Replicability

So far, we've covered making your code accessible, whether to yourself in the future (version control) or to other researchers (*remote* version control). But accessibility is only the first step. To go further, we need to make sure other researchers can understand our code.

In order learn how to write understandable code, we're going to try reproducing some figures ourselves. After cloning the LotsOBugs repo, you have a copy of all the code used to make some figures you'd like to replicate.

## Understanding the Code

Code should be understood from the top down.

### Read the README

At the top level, every project should have a file called simply ``README.'' This file should explain the purpose of the project and direct the reader to important files (e.g. the script that runs the analysis).

In this project, the README very thoughtfully explains where you can find the code that made each figure.

### Read the block comments

At the top, every file should have a block comment describing the purpose of that file. Similarly, every block of code should have a comment describing its purpose. You might ask what we mean by "block." Blocks are like paragraphs. There isn't any rule as to what constitutes a block of code. Loosely, if you can write a concise comment describing the action of some lines of code, that would make a good block. So, rather circularly, a block is a set of code that has a block comment. Separate blocks by empty lines.

In this project, the block comments provide a nice way to quickly find the code responsible for the figures in question.

### Read the inline comments

Inline comments are the most specific of comments. As a beginning programmer, you should probably use them liberally, to make explicit to yourself what each line does. However, in general inline comments should be used sparingly, if at all. Generally, if the code in a single line needs a comment to be understandable, then it's too complicated.

### Read the code

While comments are useful and necessary, you should always strive to write code that is understandable by itself. Here are a few rules of thumb to help you write understandable code:

* **Use good names**\quad
  Use function and variable names that are self-explanatory. For example, \hfill\break `random_locations_of_N_spiders_in_a_box.R` is a much better function name than `eek.R`. Don't worry about forgetting such a long name, or even typing it in. In RStudio, you can simply type `random[TAB]` to see a list of all functions you've defined whose name starts with `random`, so the extra characters don't waste time.
  * **Do one thing at a time**\quad
  This:
    ```{r,eval=FALSE}
    foo <- make_a_foo()
    king <- crown_a_foo(foo)
    dethrone_a_foo(king)
    ```
  is much more readable than `dethrone_a_foo(crown_a_foo(make_a_foo()))` or even \hfill\break
  `make_and_crown_and_dethrone_a_foo()`. Each line is a sentence, and when you do many things on the same line, you run the risk of run-on sentences.
  * **Don't use magic numbers**\quad When programmers talk about magic numbers, they don't mean 7 (necessarily).
  They mean any number that's just sitting in your code, unnamed. This is similar to "use self-explanatory variable names." `area <- 5 * 3` is not as clear as `area <- width * height`.

#### Exercise 2: Rename variables


















TODO: Maybe move the below to the README?

# A statistical interlude: Noise in biological systems
Many of the variables that we observe in biological recordings fluctuate, sometimes because we cannot control all the states of the external and internal experimental system, other times because thermal noise makes the state of the biological system we interrogate inherently variable. Examples of fluctuating quantities in biological systems include: the number of a certain type of molecule in a cell; the number of open channels in a cell; the number of electrical action potentials or "spikes" emitted by a neuron in response to a stimulus; the number of individuals in a population at a particular moment in time; the number of bacterial colonies on a plate. These are all quantities that we can make precise claims about, on average, but cannot specify with certainty for any particular experimental observation.

It is useful to model not only a mean value for a fluctuating variable, but the full shape of its distribution of values. For example, if we observe the firing of neurons in the brain to repeats of the same external stimulus, the precise times of spikes will vary between repeats. By fitting the statistics of this noise to models, we deepen our mechanistic understanding of the neural response. We can test whether or not the "noise" we observe is consistent with a truly random source of output variation, or if it has some structure that tells us about interactions between the neurons and their environment.

Often, noise in biological systems is modeled by what's called a Poisson process, whose values follow a Poisson distribution. The program you wrote to sample the bent coin lottery generated tickets whose statistics follow the binomial distribution. The binomial distribution approches the familiar Poisson distribution, in the limit of a large number of trials, $n$, or a small probability of the event, $p$, per trial:

$P_n(k)_{n\to \infty} = \frac{\lambda^k}{k!} e^{-\lambda}$

where $\lambda$ is the average rate of occurrence of our event in $n$ trials. We have just written down the Poisson distribution.  You will see this used as a model for biological variability again and again, either explicitly or implicitly.  It is important to think about whether or not it is a good model for the system under study each time you come across it or are deciding to use it for your own research. Notes on deriving the relationships between some common distributions are provided in the readings folder.

### Arthropod dispersion
In 1941 and 1942, zoologist LaMont Cole, then working at the University of Chicago, set out to survey species diversity and distribution in the woods and pastures of Kendall County, Illinois. He was particularly interested in which species co-occurred in woods versus grazing land and how their numbers varied with changes in humidity and temperature throughout the year. He laid thick oak boards in a variety of locations and counted the number of "cryptozoic" (animals found under stones, rotten logs, tree bark, etc.) individuals found under the boards several times a week, over the course of a year. For his spatial distribution studies, he aimed to determine whether arthropods distributed themselves randomly or if they had a more complex interaction pattern with each other or with their environment.



## Branching and merging [TODO: rewrite]
Most VCSs allow you to *branch* from the main development of the project (i.e., experiment freely with your project without messing up the original version). It is like saying "Save as" with a different name, but with the possibility of easily merging the two parallel versions of the file. Typical examples of a branching point in scientific projects are: a) you want to try a different angle for the Introduction of your manuscript, but you are not sure it's going to be better; b) you want to try to rewrite a piece of code, to see whether it will be faster, but surely you do not want to ruin the current version that is working just fine; c) you want to keep working on the figures, while your collaborator is editing the manuscript. In these cases, you are working on an "experimental feature", while leaving the main project unaltered (or while other people are working on it). Once you are satisfied with your changes, you would like to *merge* them with the main version of the project.

[TODO: Branch to make changes to repo]



















# References and readings
### Journal articles
All of the data used in this tutorial come from original research papers that are in the `readings` folder. Also in the `readings` folder, you will find an article by Roger Peng arguing for setting standards for reproducible research in computational science. It's a short article that we hope you will read and adopt as best practices for your own work.

### Books and tutorials
There are very many good books and tutorials on `git`. We are
particularly fond of *Pro Git*, by Scott Chacon and Ben Straub. You
can either buy a physical copy of the book, or read it online for free.

Both [GitHub](http://guides.github.com) and [Atlassian](http://atlassian.com/git/tutorials) (managing Bitbucket) have their
own tutorials.

A great way to try out Git in 15 minutes is [here](http://try.github.io).

[Software Carpentry](http://swcarpentry.github.io/git-novice) offers intensive on-site workshops and online tutorials.

### GUIs
There are several Graphical User Interfaces for [git](http://git-scm.com/downloads/guis).






























<!-- # Readability
When you begin programming, you have written just a few programs and functions and it might be possible to hold all of your naming conventions and little tricks in your mind at once. However, wait just a few days -- let alone a few months -- and you'll have a hard time deciphering, perhaps even running *your own* code if you don't write readable programs. Best practices in modern computational science also dictate that you share your code whenever you publish a result. This means that other people need to be able to read your code; it should run on other computers with other folks sending the input data and specifying or *changing* parameters. This might seem daunting, but a few simple habits will aid in keeping all of your code parsable by other humans, including your own future self.

## Commenting your code
The simplest way to make your code more readable is to write comments.
Comments begin with \# and continue to the end of the line. There are several different types of comments, and each explains what code does, how it's done, or why you did it. For our purposes, we'll divide comments into two broad categories: block comments and in-line comments. Block comments take up the whole of one or more lines and describe meaningful units comprising many lines of code. In-line comments occupy the end of a line of code, or are sandwiched between lines of code, and describe only one or few lines of code.

### Block comments

Each meaningful part of your program should have a block comment explaining the purpose of that part (e.g. functions, scripts, the whole program).

```{r,eval=FALSE}
# Modeling a Society of Foos
#
# Author:         Barry Allen
# Created:        2017 Sep 15
# Last Modified:  2017 Sep 16
#

make_a_foo <- function() {
  # Constructs a foo.
  #
  # Returns a foo object.

  return(foo)
}

crown_a_foo <- function(foo) {
  # Create a new royal foo from an ordinary foo.
  #
  # Args:
  #   foo - a foo object with no royal status
  # Returns a royal foo
  foo.is_royal = TRUE
  return(foo)
}


```

First, at the top of your file you should include a block comment that describes who wrote the code, when it was last updated, what it does, and how and why someone would use it.

Similarly, the first lines of any function should be comments. These comments should describe what the function does as well as its inputs and outputs. Ideally name and describe each of the function's arguments individually, as well as the return value. For advanced users, there are packages, e.g. `roxygen2`, that will automatically convert such comments into documentation in the `man` folder, that can then be accessed by typing `?` or `help()`.

If you are writing a script, you should include a comment at the top of every meaningful code block. These blocks are somewhat like paragraphs, and like paragraphs, there is no one right way to block your script. A good indicator of whether or not a block is a meaningful block is whether or not you can write a simple comment for that block.

Block comments should describe what you _intend_ the code to do, not what it does in detail. The former can (and should!) be done even before you write the code (like unit tests discussed in defensive programming). These comments will not only be concise, they can actually help you write better code.
Comments that describe how the code works in detail are simply repeating the actual code in words, and probably more clumsily. This is redundant; don't do it.

### In-line comments
Complex lines of code should include in-line comments to help the reader understand what the line is doing.

```{r,eval=FALSE}
z <- (x + y) / 2 # Average x and y
# Average of three averages
weird <- (((k + j) / 2) + ((x + y) / 2) + ((b + c) / 2))/3
```
Notice there is a limit to how much these in-line comments can help. They successfully translate the code into English (as opposed to simply repeating it), but absent any context, we can't understand what is happening, or why. That is what the block comments should provide.

## Writing readable code

In-line comments make complex code readable, but simple code that doesn't need comments is better by far. Whenever possible write Really Obvious Code (ROC). Tips for writing ROC:

  * **Use good names**\quad
  Use function and variable names that are self-explanatory. For example, \hfill\break `random_locations_of_N_spiders_in_a_box.R` is a much better function name than `eek.R`. Don't worry about forgetting such a long name, or even typing it in. In RStudio, you can simply type `random[TAB]` to see a list of all functions you've defined whose name starts with `random`, so the extra characters don't waste time.
  * **Do one thing at a time**\quad
  This:
    ```{r,eval=FALSE}
    foo <- make_a_foo()
    king <- crown_a_foo(foo)
    dethrone_a_foo(king)
    ```
  is much more readable than `dethrone_a_foo(crown_a_foo(make_a_foo()))` or even \hfill\break
  `make_and_crown_and_dethrone_a_foo()`. Each line is a sentence, and when you do many things on the same line, you run the risk of run-on sentences.
  * **Don't use magic numbers**\quad When programmers talk about magic numbers, they don't mean 7 (necessarily).
  They mean any number that's just sitting in your code, unnamed. This is similar to "use self-explanatory variable names." `area <- 5 * 3` is not as clear as `area <- width * height`.
 -->


<!--



#### Spiders, spiders everywhere
Load the arthropod data located in the data directory for the "Reproducibility" tutorial. If your current directory is `code`:
```{r,eval=TRUE}
x <- read.delim("../data/cole_arthropod_data_1946.csv", sep=',')
```

Have a look at the data:
```{r,eval=TRUE}
head(x)
```

We have three columns of data that correspond to $k$ arthropods found on a board, and the count, $C$, of the number of boards found at the site with $k$ arthropods. For example,
```{r,eval=TRUE}
x$k_number_of_arthropods[2]
x$C_count_of_boards_with_k_spiders[2]
```
tells you that 64 boards were found with 1 spider under them. Let's have a look at the total number of boards:
```{r,eval=TRUE}
N_boards <- sum(x$C_count_of_boards_with_k_spiders)
```

Next, let's compute the total number of spiders and the mean number of spiders per board:

```{r,eval=TRUE}
N_spiders <- sum(x$C_count_of_boards_with_k_spiders*x$k_number_of_arthropods)
spider_rate <- N_spiders/N_boards 
print(spider_rate)
```

Finally, let's plot the distrubtion of observed spiders per board. We'll want to make sure we express this as a probabilty of finding $k$ spiders on a board, so we'll have to divide by the total number of boards. 
```{r,eval=TRUE}
x$P_k_spiders <- x$C_count_of_boards_with_k_spiders/N_boards
```

Load the `ggplot2` and `cowplot` packages if you haven't already, and make a plot of $P(k)_{\textrm{spiders}}$.

```{r,eval=FALSE}
library(ggplot2)
library(cowplot)
``` 

```{r,eval=FALSE}
g_spiders <- ggplot(x, aes(x=k_number_of_arthropods, y=P_k_spiders)) +  
  geom_point(size = 3) + xlab("k spiders") + ylab("probabilty")
g_spiders
```

Now, compute the Poisson distribution of expected spiders per board if the spiders have a rate `spider_rate` and are distributed randomly across the boards. 
```{r,eval=TRUE}
lambda = spider_rate 
k <- 0:17
Poisson_k_spiders <- data.frame(k=k, probability=lambda^k*exp(-lambda)/factorial(k))
```

You can also use the built-in function `dpois` from the `stats` package to generate the probabilities in `Poisson_k_spiders`.

```{r,eval=FALSE}
Poisson_k_spiders <- data.frame(k=k, probability=dpois(k,spider_rate))
```

Add a line for the Poisson model to your plot of the observed spider probability.
```{r,eval=FALSE}
g_spiders <- g_spiders + geom_line(data=Poisson_k_spiders, aes(x=k, y=probability),  
                                   linetype='dashed', colour='#006400') +  
                         geom_point(data=Poisson_k_spiders, aes(x=k, y=probability),  
                                    colour='#006400', shape=0, size = 3)
```

> **Exercise**
>
>How well does the Poisson model account for the observed distribution of spiders? How might you quantify this? 

If the spider rate were closer to 7 spiders per board, what would you expect for the distribution of $k$ spiders per board? Let's plot this distribution. 

```{r,eval=FALSE}
Poisson_k_spiders_mean_7 <- data.frame(k=k, probability=dpois(k,7))
ggplot(x, aes(x=k_number_of_arthropods, y=P_k_spiders)) +  
  geom_line(data=Poisson_k_spiders_mean_7, aes(x=k, y=probability), linetype='dashed',   
            colour='#006400') +  
  geom_point(data=Poisson_k_spiders_mean_7, aes(x=k, y=probability), colour='#006400',  
             shape=0, size = 3) + xlab("k spiders") + ylab("probabilty")
```
Does this distribution look familiar?

#### Simulating dispersion
We know the equation for the Poisson distribution, so it was an easy task to estimate the rate of finding spiders on a board and compare our observed results to the known distribution. What if we had an underlying microscopic model that didn't have such a derivable formula? Let's explore how to simulate a random process by checking that we do in fact generate samples from a Poisson distribution when spiders are located at random positions in an area. 

> **Exercise**
> 
> Write code that randomly places 42 spiders in a $10\times 10$ area. Each $1\times 1$ box is a "board" and you have 100 boards in your simulation. Count the number of spiders per board for one run of your simulation. How does your $P_k$ distribution compare to the Poisson distirbution with rate $p=0.42$? How could you improve your simulation? Think about running many experiments or sampling a larger area. Are these equivalent? Use what you have learned about commenting your code as you write. Finally, swap code with your neighbor and see if they can run and plot your results. 

#### Sowbugs are social
Cole also counted the numbers of many other species he found under the boards. He noticed that sowbugs showed an interesting deviation from Poisson behavior. Note: sowbugs or pillbugs are land-dwelling crustaceans, more closely related to shrimp than to insects.

> **Exercise**
>
>Using what you have learned in the spiders section, compute the average rate of sowbugs per board, plot the observed probability of finding $k$ sowbugs per board, and plot the Poisson distribution with the observed sowbug rate. How well does the Poisson distribution match up with the observed sowbug distribution?

The lack of fit of the Poisson model shows that sowbugs may be more social than spiders; they are frequently found in large clusters - quantitatively more so than would be expected by chance, given their density. This is an exmaple of how comparing biological variability to what is expected given certain simple assumptions (such as "arthropods don't notice each other") can lead to biological insight. 

Cole hypothesized that sowbugs are attracted to one another, and prefer to aggregate into groups of 3 or more individuals. He further hypothesized that these aggregates disperse themselves randomly. The formation of the aggregate is a random process, as is the dispersion of the aggregates. This is an example of a doubly-stochastic process. 

In the late 1970's, Janardan and colleagues showed that the count distribution of sowbugs and other organisms with social interactions can be well-described by a modified Poisson process, a process resulting in a Lagrangian Poisson Distribution or LGP, that accounts for the attraction or repulsion of individuals to each other. The LGP has a mean rate parameter, $\lambda_1$, just like in the Poisson distribution. The LGP has a second parameter, $\lambda_2$, that describes the deviation from expected Poisson dispersion, or the ratio of the variance of the distribution to the mean. 

>NB: A Poisson distribution has a variance-to-mean ratio equal to 1. As an exercise, show that the variance of the spider count distribution is approximately equal to its mean.

$\lambda_2 > 0$ implies that organisms are over-dispersed and may be attracted to one another, forming more large-number clusters than expected by chance. $\lambda_2 < 0$ implies that organisms are under-dispersed and are likely repulsed by one another, resulting in a more even distribution across space and, hence, lower count variance. The LGP distribution, in terms of these two paramters, is

$P(k) = \lambda_1 (\lambda_1 + k\lambda_2)^{k-1}\frac{e^{-(\lambda_1+k\lambda_2)}}{k!}.$

When $\lambda_2 =0$, we get precisely the Poisson distribution. Back to the sowbugs! From an empiral fit, we find that $\lambda_2 = 0.53214$ produces the best agreement between this model and the data. A simple estimate of $\lambda_1$ follows from 

$\lambda_1 = \textrm{mean} (1 - \lambda_2)$.

```{r,eval=TRUE}
mean_sowbugs <- sum(x$k_number_of_arthropods*x$C_count_of_boards_with_k_sowbugs) /  
  sum(x$C_count_of_boards_with_k_sowbugs)
lambda2_sowbugs = 0.53214
lambda1_sowbugs = mean_sowbugs*(1-lambda2_sowbugs)
```

We can now use these parameters to show how well this model fits the sowbug data. First let's load a package that contains a function for the LGP equation, that returns a count distribution over a range of $k$ values, given input parameters $\lambda_1$ and $\lambda_2$.

```{r,eval=FALSE}
library(RMKdiscrete)
LGP_k_sowbugs <- data.frame(k=k, probability=dLGP(k,lambda1_sowbugs,lambda2_sowbugs))
```

> **Exercise**
>
> Add a line to your sowbug plot, showing the output of the LGP function with our defined parameters. How well does this model fit the data?

#### Weevils are antisocial
In 1975, Rodger Mitchell reported on data from an experiment on weevil egg-laying behavior. Weevils lay their eggs in mung beans, then the larva hatches within the bean and eats the bean as it matures. A larger bean with fewer other hatching eggs to compete with will increase the larva's chance of survival. Mitchell measured how many eggs were laid per bean in an experiment were eggs were plentiful and competition was low. Load the bean weevil data 
```{r,eval=FALSE}
weevil_data <- read.delim("../data/mitchell_weevil_egg_data_1975.csv", sep=',')
N_eggs <- sum(weevil_data$C_count_of_beans_with_k_eggs)
weevil_data$P_k_weevils <- weevil_data$C_count_of_beans_with_k_eggs/N_eggs
```

$\ldots$and plot it. 
```{r,eval=FALSE}
g_weevils <- ggplot(weevil_data, aes(x=k_number_of_eggs, y=P_k_weevils)) +  
  geom_point(size = 3) + xlab("k spiders") + ylab("probabilty")
g_weevils
```

> **Exercise**
>
> Add a line to your plot that shows a Poisson distribution with the same mean number of eggs per bean. How does the Poisson fit compare to the data? Is the variance higher or lower, by eye, than for the Poisson distribution? 

With these data, it's possible to estimate both $\lambda_1$ and $\lambda_2$ from the data and get a good fit of the LGP. You can estimate 

$\lambda_2 = 1 - (\frac{\textrm{variance}}{\textrm{mean}})^{-1/2}$, and
$\lambda_1 = \textrm{mean} (1 - \lambda_2)$.

> **Exercise**
>
> Compute these two parameters. Use `dLPG` and add a line to your weevil plot for these results. Does this distribution fit the data well? 

-->



# Data Challenge

TODO

Make a pull request!

It should do X, Y, Z (tasks for 8 people)

The requirements are:

* It should include a commit from every group member.
* There should be at least one merge (either due to merging branches, or because of a merge conflict)