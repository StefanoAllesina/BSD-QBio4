---
title: "Reproducibility of data analysis"
author: "Stephanie Palmer & Stefano Allesina & Graham Smith"
date: "September 12-14, 2017"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
urlcolor: blue
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  eval      = FALSE,
  comment   = "#",
  results   = "hold",
  # collapse  = TRUE,
  fig.align = "center")
```

# Goals
This tutorial will cover methods for making your code reproducible, both by you and by others.
Producing results that are reproducible by others is the very essence of science, and writing code that is reproducible by **you** is just the first step.
In particular, we will discuss methods to ensure that you can:

* reproduce your calculations precisely,
* make your code readable,
* track changes to your code,
* make your code freely accessible to the wider world via [github](http://github.com/).

Along the way, you will get an introduction to stochastic processes and how they are used to model biological variability.
By the end of this tutorial, you should know why it is important to save your seeds and merge your branches.
You should also know why reproducible coding practices can help you *now*, even if (indeed, particularly if) you are just learning to code.

<!-- TODO: Are stochastic processes still present? -->

## Installation notes
For this tutorial, relatively up-to-date versions of `R` and `RStudio` are needed. To install `R`, follow instructions at [`cran.rstudio.com`](https://cran.rstudio.com/). Then install `Rstudio` following the instructions at [`goo.gl/a42jYE`](https://www.rstudio.com/products/rstudio/download2/). Download the `ggplot2`, `cowplot`, `stats`, and `RMKdiscrete` packages. You should also install the UNIX utility `git` for [OSX](https://git-scm.com/download/mac) or [Windows](https://git-for-windows.github.io/).






# Introduction to `git`
`git` is a version control system. Version control is a way to record and organize changes to a project that consists of files and directories. Over time, a version control system like `git` builds a repository of the whole history of your project.

When you start working on a new project, you tell `git` what directory will contain the project. Then `git` takes a snapshot of that directory to create the beginning of your repository. If the project is brand new, this may be a snapshot of an empty directory.  After you've done some work, you can tell `git` to take a new snapshot, called a **commit**. All snapshots remain available---you can always recover previously committed versions of files.

`git` is especially important for collaborative projects: everybody can simultaneously work on the project, even on the same file. Conflicting changes are reported, and can be managed using side-by-side comparisons. The possibility of **branching** allows you to experiment with changes (e.g. shall we rewrite the introduction of this paper?), and then decide whether to **merge** them into the project.

### Why use Version Control?
If you ever collaborate on writing code, then version control is for you. If you ever horribly break your project and need to move quickly back in time two days (or two weeks), then version control is for you. If you ever need to replicate a figure you made three months ago, version control is for you (I *guarantee* you will need to do this at some point). If you ever need to share your code publicly, then version control is for you. Most likely, version control is for you.

Version control is useful for small projects, and is essential for large collaborative projects. It vastly improves the workflow, efficiency and reproducibility. Without it, it is quite easy to lose track of the status of a manuscript (who has the most recent version?), or lose time (I cannot proceed with my changes, because I have to wait for the edits of my collaborators).

Many scientists keep backup versions of the same project over time or append a date/initials to different versions of the same file (e.g. various drafts of the same manuscript). This manual approach quickly becomes unmanageable, with simply too many files and versions to keep track of in a timely and organized manner. Version control allows you to access all previously committed versions of the files and directories of your project. This means that it is quite easy to undo short-term changes: Bad day? Just go back to yesterday's version! You can also access previous stages of the project: "I need to access the manuscript's version and all the analysis files in exactly the state that they where in when I sent the draft for review three months ago." Checking out an entire project at a certain point in time is easy with a version control system but much more difficult with Dropbox or Google Drive.

Version control makes it trivial to host your code publicly (e.g. on Github or Bitbucket) and to share a robust link to your code in any publication.

Version control might look like overkill at first. However, with a little bit of practice you will automatically run a few commands before you start working on a project, and again once you are done working on it. A small price to pay, considering the advantages. Simply put, using version control makes you a more organized and efficient scientist.

> Stefano's testimonial: "Our laboratory adopted version control for all our projects in 2010, and sometimes we wonder how we managed without it."






<!-- ## Git
For this introduction to version control, we use `git`, which is one of the most popular version control systems. `git` is also free software, and is available for all computer architectures. Many good tutorials are available online, and many websites will host your repositories for free.

Other options that exist are `Mercurial` (very similar to `git`) and Subversion Version Control `svn`, which is an older system, but still widespread.

There are two main paradigms for VCSs allowing multiple users to collaborate: in a *centralized* VCS (e.g., `svn`), the whole history of a project is stored exclusively on a server, and users download the most current snapshot of the project, modify it, and send the changes to the server; in a *distributed* VCS (e.g., `git`), the complete history of the repository is stored on each user's computer.

`git` was initially developed by Linus Torvalds (the "Linu" in Linux), exactly for the development of the Linux kernel. It was first released in 2005 and has since become the most widely adopted version control system.

At first, we work with *local* repositories, meaning that all the files are stored exclusively on your computer. `git` by default works with local repositories. Later, we introduce *remote* repositories, which are also hosted on a web server, making it easy for you to share your projects with others (or work on the same project from different computers). These remote repositories can be hosted on such well known sites as Github and Bitbucket, and `git` will play nicely. -->

<!-- ### Configuring git
First, open a terminal window.

The first time you use `git`, (or whenever you install `git` on a new computer), you need to set up the environment. To store your preferred user name and email, type:

```{r, engine = 'bash', eval = FALSE}
$ git config --global user.name "Charles Darwin"
$ git config --global user.email crdarwin@royalsociety.org
```

You can set up your preferred text editor, which will be used to write the messages associated with your commits. If you are familiar with text editors and already have a favorite (e.g. `emacs` or `vim`), feel free to use it. Otherwise, Window users please use `notepad` and Mac users please use `nano`.

```{r, engine = 'bash', eval = FALSE}
$ git config --global core.editor nano
```
Again, Windows users please replace `nano` with `notepad` in the above.

To check all of your settings and see all available options, type:
```{r, engine = 'bash', eval = FALSE}
$ git config --list
``` -->

## Help using GitKraken and `git`

GitKraken is a graphical user interface (GUI) on top of the older command-line interface (CLI) `git`. If you've never used `git` before, GitKraken has a good tutorial: [https://support.gitkraken.com/start-here/guide](https://support.gitkraken.com/start-here/guide)

## Glossary

  - `commit`: *v.* Take a snapshot of the current progress. *n.* A snapshot of the project at a particular point in time
  - **commit graph**: TODO
  - `stage`: *v.* Add changes to the set that will be committed. (does not commit!)
  - `repository` (`repo`): *n.* Entire history of project. Set of all commits on all branches.
  - `branch`: *n.* A lineage of commits. A repository can have multiple branches, and committing changes to one will not affect the others. You can switch between branches (see `checkout`). When you switch branches, all your project's files change to the state they were in at the last commit on that branch. *v.* To create a new branch.
  - `checkout`: **TODO**


## Workflow

 TODO: fix numbers
 1) Pull any new changes from your collaborators
 1) Work on your project
 2) When you've done something meaningful, stage your changes
 3) Commit your changes, writing a meaningful commit message
 4) Repeat!
 5) When you're done for the day, push your changes to the remote repository (e.g. on GitHub).

# First Steps: Your First Repo

Let's `init` your first repo from scratch! To do so, in GitKraken select `File > Init Repo`. We'll be creating a "Local Only" repository for this simple example. That just means we'll not be putting any files on any website (such as GitHub). The files are only stored on your computer. Browse for [TODO: Place to put files].

Great! Now we have an empty repository. Let's put something in it. Using a text editor of your choice (Notepad, TextEdit, Sublime, even RStudio), create a file called `origin.txt` with the text "An abstract of an Essay on the Origin of Species...". Save it.

Now go back to GitKraken. At the top of the center panel, an entry has been added, `// WIP`. This stands for "Work In Progress" and it indicates you've made changes to your project that have not been committed to the repository. We have two steps to record the changes to the repository: First, we stage the changes. Second, we commit the staged changes.

TODO: Staging and committing

If you use `git` in the wild, you'll find that [TODO: This is everything]

# A Deeper Example

For the remainder of the tutorial, we'll be using a more complicated example based in code hosted on GitHub. TODO

## Clone a Repo

The previous section covered how to create your own repo from scratch [TODO: common use case: use someone else's code]

[TODO: clone LotsOBugs repo]

## Remote repositories
So far, we have been working with local repositories, hosted in only one computer. When collaborating (or when working on the same project from different computers), it is convenient to host the repository on a server, so that the collaborators can see your changes, and add
their own.

There are two main options: a) setup your own server (this is beyond the scope of this introduction); b) host your repositories on a website offering this service (for a fee, or for free).

The most popular option for hosting open source projects, where the whole world can see what and when you commit, is [GitHub](http://github.com), which will store all your public repositories for free. In fact, as as student you can get private repositories for free! The [Student Developer Pack](https://education.github.com/pack) comes with access to lots of other goodies, but private repositories make it a necessity. They're useful in the early stages of your project when you're paranoid about anyone, let alone the entire world, seeing your hasty hacks. But remember to publicize your repository when you publish! (Note: probably still not HIPAA compliant)

After the initial setup (which is specific to the service you use, and thus not treated here), you only need to add two new commands to your `git` workflow: `pull` and `push`. When you want to work on a project that is tracked by a remote repository, you `pull` the most recent version from the server and work on your local copy using the commands illustrated above. When you are done, you `push` your commits to the server so that other users can see them. Your workflow will look like:

## Understanding the Code

TODO: Maybe move the below to the README?

### Noise in biological systems
Many of the variables that we observe in biological recordings fluctuate, sometimes because we cannot control all the states of the external and internal experimental system, other times because thermal noise makes the state of the biological system we interrogate inherently variable. Examples of fluctuating quantities in biological systems include: the number of a certain type of molecule in a cell; the number of open channels in a cell; the number of electrical action potentials or "spikes" emitted by a neuron in response to a stimulus; the number of individuals in a population at a particular moment in time; the number of bacterial colonies on a plate. These are all quantities that we can make precise claims about, on average, but cannot specify with certainty for any particular experimental observation.

It is useful to model not only a mean value for a fluctuating variable, but the full shape of its distribution of values. For example, if we observe the firing of neurons in the brain to repeats of the same external stimulus, the precise times of spikes will vary between repeats. By fitting the statistics of this noise to models, we deepen our mechanistic understanding of the neural response. We can test whether or not the "noise" we observe is consistent with a truly random source of output variation, or if it has some structure that tells us about interactions between the neurons and their environment.

Often, noise in biological systems is modeled by what's called a Poisson process, whose values follow a Poisson distribution. The program you wrote to sample the bent coin lottery generated tickets whose statistics follow the binomial distribution. The binomial distribution approches the familiar Poisson distribution, in the limit of a large number of trials, $n$, or a small probability of the event, $p$, per trial:

$P_n(k)_{n\to \infty} = \frac{\lambda^k}{k!} e^{-\lambda}$

where $\lambda$ is the average rate of occurrence of our event in $n$ trials. We have just written down the Poisson distribution.  You will see this used as a model for biological variability again and again, either explicitly or implicitly.  It is important to think about whether or not it is a good model for the system under study each time you come across it or are deciding to use it for your own research. Notes on deriving the relationships between some common distributions are provided in the readings folder.

### Arthropod dispersion
In 1941 and 1942, zoologist LaMont Cole, then working at the University of Chicago, set out to survey species diversity and distribution in the woods and pastures of Kendall County, Illinois. He was particularly interested in which species co-occurred in woods versus grazing land and how their numbers varied with changes in humidity and temperature throughout the year. He laid thick oak boards in a variety of locations and counted the number of "cryptozoic" (animals found under stones, rotten logs, tree bark, etc.) individuals found under the boards several times a week, over the course of a year. For his spatial distribution studies, he aimed to determine whether arthropods distributed themselves randomly or if they had a more complex interaction pattern with each other or with their environment.


## Read the README

## Read the block comments

## Read the inline comments

## Read the code



## Branching and merging [TODO: rewrite]
Most VCSs allow you to *branch* from the main development of the project (i.e., experiment freely with your project without messing up the original version). It is like saying "Save as" with a different name, but with the possibility of easily merging the two parallel versions of the file. Typical examples of a branching point in scientific projects are: a) you want to try a different angle for the Introduction of your manuscript, but you are not sure it's going to be better; b) you want to try to rewrite a piece of code, to see whether it will be faster, but surely you do not want to ruin the current version that is working just fine; c) you want to keep working on the figures, while your collaborator is editing the manuscript. In these cases, you are working on an "experimental feature", while leaving the main project unaltered (or while other people are working on it). Once you are satisfied with your changes, you would like to *merge* them with the main version of the project.

[TODO: Branch to make changes to repo]



















## References and readings
### Journal articles
All of the data used in this tutorial come from original research papers that are in the `readings` folder. Also in the `readings` folder, you will find an article by Roger Peng arguing for setting standards for reproducible research in computational science. It's a short article that we hope you will read and adopt as best practices for your own work.

### Books and tutorials
There are very many good books and tutorials on `git`. We are
particularly fond of *Pro Git*, by Scott Chacon and Ben Straub. You
can either buy a physical copy of the book, or read it online for free.

Both [GitHub](http://guides.github.com) and [Atlassian](http://atlassian.com/git/tutorials) (managing Bitbucket) have their
own tutorials.

A great way to try out Git in 15 minutes is [here](http://try.github.io).

[Software Carpentry](http://swcarpentry.github.io/git-novice) offers intensive on-site workshops and online tutorials.

### GUIs
There are several Graphical User Interfaces for [git](http://git-scm.com/downloads/guis).






























# Readability
<!-- TODO: Include story metaphor? -->
When you begin programming, you have written just a few programs and functions and it might be possible to hold all of your naming conventions and little tricks in your mind at once. However, wait just a few days -- let alone a few months -- and you'll have a hard time deciphering, perhaps even running *your own* code if you don't write readable programs. Best practices in modern computational science also dictate that you share your code whenever you publish a result. This means that other people need to be able to read your code; it should run on other computers with other folks sending the input data and specifying or *changing* parameters. This might seem daunting, but a few simple habits will aid in keeping all of your code parsable by other humans, including your own future self.

## Commenting your code
The simplest way to make your code more readable is to write comments.
Comments begin with \# and continue to the end of the line. There are several different types of comments, and each explains what code does, how it's done, or why you did it. For our purposes, we'll divide comments into two broad categories: block comments and in-line comments. Block comments take up the whole of one or more lines and describe meaningful units comprising many lines of code. In-line comments occupy the end of a line of code, or are sandwiched between lines of code, and describe only one or few lines of code.

### Block comments

Each meaningful part of your program should have a block comment explaining the purpose of that part (e.g. functions, scripts, the whole program).

```{r,eval=FALSE}
# Modeling a Society of Foos
#
# Author:         Barry Allen
# Created:        2017 Sep 15
# Last Modified:  2017 Sep 16
#

make_a_foo <- function() {
  # Constructs a foo.
  #
  # Returns a foo object.

  return(foo)
}

crown_a_foo <- function(foo) {
  # Create a new royal foo from an ordinary foo.
  #
  # Args:
  #   foo - a foo object with no royal status
  # Returns a royal foo
  foo.is_royal = TRUE
  return(foo)
}


```

First, at the top of your file you should include a block comment that describes who wrote the code, when it was last updated, what it does, and how and why someone would use it.

Similarly, the first lines of any function should be comments. These comments should describe what the function does as well as its inputs and outputs. Ideally name and describe each of the function's arguments individually, as well as the return value. For advanced users, there are packages, e.g. `roxygen2`, that will automatically convert such comments into documentation in the `man` folder, that can then be accessed by typing `?` or `help()`.

If you are writing a script, you should include a comment at the top of every meaningful code block. These blocks are somewhat like paragraphs, and like paragraphs, there is no one right way to block your script. A good indicator of whether or not a block is a meaningful block is whether or not you can write a simple comment for that block.

Block comments should describe what you _intend_ the code to do, not what it does in detail. The former can (and should!) be done even before you write the code (like unit tests discussed in defensive programming). These comments will not only be concise, they can actually help you write better code. <!-- TODO: explain -->
Comments that describe how the code works in detail are simply repeating the actual code in words, and probably more clumsily. This is redundant; don't do it.

### In-line comments
Complex lines of code should include in-line comments to help the reader understand what the line is doing.

```{r,eval=FALSE}
z <- (x + y) / 2 # Average x and y
# Average of three averages
weird <- (((k + j) / 2) + ((x + y) / 2) + ((b + c) / 2))/3
```
Notice there is a limit to how much these in-line comments can help. They successfully translate the code into English (as opposed to simply repeating it), but absent any context, we can't understand what is happening, or why. That is what the block comments should provide.
<!-- TODO: use inline comments to segue into ROC -->

## Writing readable code

In-line comments make complex code readable, but simple code that doesn't need comments is better by far. Whenever possible write Really Obvious Code (ROC). Tips for writing ROC:

  * **Use good names**\quad
  Use function and variable names that are self-explanatory. For example, \hfill\break `random_locations_of_N_spiders_in_a_box.R` is a much better function name than `eek.R`. Don't worry about forgetting such a long name, or even typing it in. In RStudio, you can simply type `random[TAB]` to see a list of all functions you've defined whose name starts with `random`, so the extra characters don't waste time.
  * **Do one thing at a time**\quad
  This:
    ```{r,eval=FALSE}
    foo <- make_a_foo()
    king <- crown_a_foo(foo)
    dethrone_a_foo(king)
    ```
  is much more readable than `dethrone_a_foo(crown_a_foo(make_a_foo()))` or even \hfill\break
  `make_and_crown_and_dethrone_a_foo()`. Each line is a sentence, and when you do many things on the same line, you run the risk of run-on sentences.
  * **Don't use magic numbers**\quad When programmers talk about magic numbers, they don't mean 7 (necessarily).
  They mean any number that's just sitting in your code, unnamed. This is similar to "use self-explanatory variable names." `area <- 5 * 3` is not as clear as `area <- width * height`.



<!-- # DIY lottery
To practice writing readable code, let's write a short program to generate a draw from a lottery. The lottery is a somewhat peculiar one: Each ticket for the lottery is a buyer-determined string of $30$ $1$'s and $0$'s. The winning lottery number is drawn by flipping a coin 30 times and reporting a "$1$" for heads and a "$0$" for tails. The same bent coin is always used, and it has a probability of $0.1$ of turning up heads and a probability of $0.9$ of turning up tails. The prize money for the lottery is $\$10,000,000$, and each ticket costs only \$1.

Let's make sure to load the `stats` package.
```{r,eval=TRUE}
library(stats)
```

Next, let's explore how we simulate flipping this bent coin once. We're going to use one of R's random number generator functions `runif`, which will give you uniformly distributed numbers on the interval $[0.1]$. Here's one way to do it:
```{r,eval=TRUE}
# define the probability of heads
p_heads <- 0.1
# draw a random number from the interval [0,1]
random_val <- runif(1)
# threshold the random number based on p_heads to decide the sign of the flip
if (random_val < p_heads) {
  coinflip <- 1
} else {
  coinflip <- 0
}
# 1 is for heads, 0 is for tails
```
Can you see any ways to make this code even more Really Obvious?

> **Exercise**
>
> Write a function in R that will simulate one random draw for the winning lottery ticket. Let the inputs to the function be the number of digits in the lottery ticket (default value $10$) and the probability of heads (default value $0.1$). The function should output a sequence of $1$'s and $0$'s. Use all of the best practices for commenting your code.

Each flip of a coin like this with probability, $p$, of heads is an example of a Bernoulli trial, which is the technical term for an experiment with only two output states, success or failure. The number of heads in the sequence of independent coin flips generated by our lottery will follow a binomial distribution

$P_n(k) = \binom{n}{k} p^k (1-p)^{n-k},$

where $p$ is the probability of heads ($1$'s), $n$ is the length of our lottery ticket, and $k$ is the number of heads in the ticket. The prefactor $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ is called the binomial coefficient and describes the number of unique ways of placing $k$ identical objects in $n$ bins. Capital "$P_n$" represents the probability distribution of $k$ heads out of $n$ tosses.

You can use the function `rbinom` to generate draws from a Bernoulli distribution, since a Bernoulli trial is just a binomial trial with only one coin flip. In our lottery, this code would amount to flipping the coin $n_\textrm{flips}$ times.
```{r,eval=TRUE}
n_flips <- 10
rbinom(n_flips,1,p_heads)
```

> **Exercise**
>
> If you could only buy one ticket for this lottery, which one would you buy? Modify your code (perhaps use `rbinom`) and run it several times (maybe 100 or even 1000) to sample from lots of possible lottery outcomes. Did you pick a good number? How many times did your number come up?

## Save your seeds
Compare a draw from your lottery with your neighbor. Do you draw the same sequence of random lottery tickets? Why not? If you wanted to reproduce the *exact* same output from your lottery each time you decide to reset it, you'll need to know a little more about how R's random number generator (RNG) works. Try typing:

```{r,eval=FALSE}
? RNG
```

That should open documentation in the "Help" pane. You will notice that the function `RNGkind` is the interface for querying the current state of the RNG. Let's find out what the current settings are:

```{r,eval=TRUE}
RNGkind()
```

The first part is the RNG algorithm, the second specifies the algorithm for transforming uniformly distributed random numbers into random samples from the normal, or Gaussian, distribution. The twister algorithm based on Mersenne prime numbers, $M_n = 2^n -1$, where $n$ is also prime, is a state-of-the-art pseudo-random number generation scheme, developed by Matsumoto and Nishimura in 1997. NB: RNG's should technically be called *pseudo*-random number generators or PRNG's, in part because they all have some period after which they will produce exactly the same sequence. The trick is to find an algorithm with a period so long you'll never notice the "P" in the "PRNG". The Mersenne Twister algorithm has a period of $2^{19937}-1$ and passes many statistical tests for randomness.

The seed to an RNG is usually a large integer that provides an initialization the RNG algorithm. Scrolling down to "Note" in the "Help" pane, you'll learn that the seed to `R`'s RNG is set by the current time and the process ID. That means that your simulation results will depend on when you start your `R` session, run your code, and even some local information in your processing environment. Compare the output of `runif` with your neighbor.

```{r,eval=FALSE}
runif(5)
```

Starting an RNG with the same seed will produce exactly the same sequence of random numbers; an RNG spits out random numbers, but not noisy ones. To reproduce your simulation results precisely when you use an RNG, you'll want control of that seed. `R` uses `set.seed` which takes a small integer as input and generates, deterministically, all the random seeds necessary for your RNG algorithm.

```{r,eval=FALSE}
set.seed(19937)
runif(5)
runif(5)
set.seed(19937)
runif(5)
```

> **Exercise**
>
>Test whether or not you and your neighbor get precisely the same sequence of numbers when you use the same seed. Will this work if you use different RNG algorithms? Try changing your RNG algorirhm using `RNGkind` and compare your results with your neighbor, when you use the same seed. If you wanted to be able to instruct someone to reproduce your exact simulation results using `R`'s RNG, what would you need to tell them?

You can use `RNGkind` to set or query both the RNG and normal algorithms. You can save this information along with the current value of `seed` using something like:

```{r,eval=FALSE}
seed <- 19937
set.seed(seed)
seed_used <- seed
RNGkind_used <- RNGkind()
save("seed_used","RNGkind_used",file=RNGinfo_for_mycode)
```

When you are ready to share your code with others, you should also save all the version information for your current `R` package and libraries to this same file. -->

<!--



#### Spiders, spiders everywhere
Load the arthropod data located in the data directory for the "Reproducibility" tutorial. If your current directory is `code`:
```{r,eval=TRUE}
x <- read.delim("../data/cole_arthropod_data_1946.csv", sep=',')
```

Have a look at the data:
```{r,eval=TRUE}
head(x)
```

We have three columns of data that correspond to $k$ arthropods found on a board, and the count, $C$, of the number of boards found at the site with $k$ arthropods. For example,
```{r,eval=TRUE}
x$k_number_of_arthropods[2]
x$C_count_of_boards_with_k_spiders[2]
```
tells you that 64 boards were found with 1 spider under them. Let's have a look at the total number of boards:
```{r,eval=TRUE}
N_boards <- sum(x$C_count_of_boards_with_k_spiders)
```

Next, let's compute the total number of spiders and the mean number of spiders per board:

```{r,eval=TRUE}
N_spiders <- sum(x$C_count_of_boards_with_k_spiders*x$k_number_of_arthropods)
spider_rate <- N_spiders/N_boards 
print(spider_rate)
```

Finally, let's plot the distrubtion of observed spiders per board. We'll want to make sure we express this as a probabilty of finding $k$ spiders on a board, so we'll have to divide by the total number of boards. 
```{r,eval=TRUE}
x$P_k_spiders <- x$C_count_of_boards_with_k_spiders/N_boards
```

Load the `ggplot2` and `cowplot` packages if you haven't already, and make a plot of $P(k)_{\textrm{spiders}}$.

```{r,eval=FALSE}
library(ggplot2)
library(cowplot)
``` 

```{r,eval=FALSE}
g_spiders <- ggplot(x, aes(x=k_number_of_arthropods, y=P_k_spiders)) +  
  geom_point(size = 3) + xlab("k spiders") + ylab("probabilty")
g_spiders
```

Now, compute the Poisson distribution of expected spiders per board if the spiders have a rate `spider_rate` and are distributed randomly across the boards. 
```{r,eval=TRUE}
lambda = spider_rate 
k <- 0:17
Poisson_k_spiders <- data.frame(k=k, probability=lambda^k*exp(-lambda)/factorial(k))
```

You can also use the built-in function `dpois` from the `stats` package to generate the probabilities in `Poisson_k_spiders`.

```{r,eval=FALSE}
Poisson_k_spiders <- data.frame(k=k, probability=dpois(k,spider_rate))
```

Add a line for the Poisson model to your plot of the observed spider probability.
```{r,eval=FALSE}
g_spiders <- g_spiders + geom_line(data=Poisson_k_spiders, aes(x=k, y=probability),  
                                   linetype='dashed', colour='#006400') +  
                         geom_point(data=Poisson_k_spiders, aes(x=k, y=probability),  
                                    colour='#006400', shape=0, size = 3)
```

> **Exercise**
>
>How well does the Poisson model account for the observed distribution of spiders? How might you quantify this? 

If the spider rate were closer to 7 spiders per board, what would you expect for the distribution of $k$ spiders per board? Let's plot this distribution. 

```{r,eval=FALSE}
Poisson_k_spiders_mean_7 <- data.frame(k=k, probability=dpois(k,7))
ggplot(x, aes(x=k_number_of_arthropods, y=P_k_spiders)) +  
  geom_line(data=Poisson_k_spiders_mean_7, aes(x=k, y=probability), linetype='dashed',   
            colour='#006400') +  
  geom_point(data=Poisson_k_spiders_mean_7, aes(x=k, y=probability), colour='#006400',  
             shape=0, size = 3) + xlab("k spiders") + ylab("probabilty")
```
Does this distribution look familiar?

#### Simulating dispersion
We know the equation for the Poisson distribution, so it was an easy task to estimate the rate of finding spiders on a board and compare our observed results to the known distribution. What if we had an underlying microscopic model that didn't have such a derivable formula? Let's explore how to simulate a random process by checking that we do in fact generate samples from a Poisson distribution when spiders are located at random positions in an area. 

> **Exercise**
> 
> Write code that randomly places 42 spiders in a $10\times 10$ area. Each $1\times 1$ box is a "board" and you have 100 boards in your simulation. Count the number of spiders per board for one run of your simulation. How does your $P_k$ distribution compare to the Poisson distirbution with rate $p=0.42$? How could you improve your simulation? Think about running many experiments or sampling a larger area. Are these equivalent? Use what you have learned about commenting your code as you write. Finally, swap code with your neighbor and see if they can run and plot your results. 

#### Sowbugs are social
Cole also counted the numbers of many other species he found under the boards. He noticed that sowbugs showed an interesting deviation from Poisson behavior. Note: sowbugs or pillbugs are land-dwelling crustaceans, more closely related to shrimp than to insects.

> **Exercise**
>
>Using what you have learned in the spiders section, compute the average rate of sowbugs per board, plot the observed probability of finding $k$ sowbugs per board, and plot the Poisson distribution with the observed sowbug rate. How well does the Poisson distribution match up with the observed sowbug distribution?

The lack of fit of the Poisson model shows that sowbugs may be more social than spiders; they are frequently found in large clusters - quantitatively more so than would be expected by chance, given their density. This is an exmaple of how comparing biological variability to what is expected given certain simple assumptions (such as "arthropods don't notice each other") can lead to biological insight. 

Cole hypothesized that sowbugs are attracted to one another, and prefer to aggregate into groups of 3 or more individuals. He further hypothesized that these aggregates disperse themselves randomly. The formation of the aggregate is a random process, as is the dispersion of the aggregates. This is an example of a doubly-stochastic process. 

In the late 1970's, Janardan and colleagues showed that the count distribution of sowbugs and other organisms with social interactions can be well-described by a modified Poisson process, a process resulting in a Lagrangian Poisson Distribution or LGP, that accounts for the attraction or repulsion of individuals to each other. The LGP has a mean rate parameter, $\lambda_1$, just like in the Poisson distribution. The LGP has a second parameter, $\lambda_2$, that describes the deviation from expected Poisson dispersion, or the ratio of the variance of the distribution to the mean. 

>NB: A Poisson distribution has a variance-to-mean ratio equal to 1. As an exercise, show that the variance of the spider count distribution is approximately equal to its mean.

$\lambda_2 > 0$ implies that organisms are over-dispersed and may be attracted to one another, forming more large-number clusters than expected by chance. $\lambda_2 < 0$ implies that organisms are under-dispersed and are likely repulsed by one another, resulting in a more even distribution across space and, hence, lower count variance. The LGP distribution, in terms of these two paramters, is

$P(k) = \lambda_1 (\lambda_1 + k\lambda_2)^{k-1}\frac{e^{-(\lambda_1+k\lambda_2)}}{k!}.$

When $\lambda_2 =0$, we get precisely the Poisson distribution. Back to the sowbugs! From an empiral fit, we find that $\lambda_2 = 0.53214$ produces the best agreement between this model and the data. A simple estimate of $\lambda_1$ follows from 

$\lambda_1 = \textrm{mean} (1 - \lambda_2)$.

```{r,eval=TRUE}
mean_sowbugs <- sum(x$k_number_of_arthropods*x$C_count_of_boards_with_k_sowbugs) /  
  sum(x$C_count_of_boards_with_k_sowbugs)
lambda2_sowbugs = 0.53214
lambda1_sowbugs = mean_sowbugs*(1-lambda2_sowbugs)
```

We can now use these parameters to show how well this model fits the sowbug data. First let's load a package that contains a function for the LGP equation, that returns a count distribution over a range of $k$ values, given input parameters $\lambda_1$ and $\lambda_2$.

```{r,eval=FALSE}
library(RMKdiscrete)
LGP_k_sowbugs <- data.frame(k=k, probability=dLGP(k,lambda1_sowbugs,lambda2_sowbugs))
```

> **Exercise**
>
> Add a line to your sowbug plot, showing the output of the LGP function with our defined parameters. How well does this model fit the data?

#### Weevils are antisocial
In 1975, Rodger Mitchell reported on data from an experiment on weevil egg-laying behavior. Weevils lay their eggs in mung beans, then the larva hatches within the bean and eats the bean as it matures. A larger bean with fewer other hatching eggs to compete with will increase the larva's chance of survival. Mitchell measured how many eggs were laid per bean in an experiment were eggs were plentiful and competition was low. Load the bean weevil data 
```{r,eval=FALSE}
weevil_data <- read.delim("../data/mitchell_weevil_egg_data_1975.csv", sep=',')
N_eggs <- sum(weevil_data$C_count_of_beans_with_k_eggs)
weevil_data$P_k_weevils <- weevil_data$C_count_of_beans_with_k_eggs/N_eggs
```

$\ldots$and plot it. 
```{r,eval=FALSE}
g_weevils <- ggplot(weevil_data, aes(x=k_number_of_eggs, y=P_k_weevils)) +  
  geom_point(size = 3) + xlab("k spiders") + ylab("probabilty")
g_weevils
```

> **Exercise**
>
> Add a line to your plot that shows a Poisson distribution with the same mean number of eggs per bean. How does the Poisson fit compare to the data? Is the variance higher or lower, by eye, than for the Poisson distribution? 

With these data, it's possible to estimate both $\lambda_1$ and $\lambda_2$ from the data and get a good fit of the LGP. You can estimate 

$\lambda_2 = 1 - (\frac{\textrm{variance}}{\textrm{mean}})^{-1/2}$, and
$\lambda_1 = \textrm{mean} (1 - \lambda_2)$.

> **Exercise**
>
> Compute these two parameters. Use `dLPG` and add a line to your weevil plot for these results. Does this distribution fit the data well? 

-->



## Data Challenge

TODO

Make a pull request!

It should do X, Y, Z (tasks for 8 people)

The requirements are:

* It should include a commit from every group member.
* There should be at least one merge (either due to merging branches, or because of a merge conflict)